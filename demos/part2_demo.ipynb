{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data and Split Rows for Win/Loss Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fight Data Stats: \n",
      "Shape:  (5062, 147)\n",
      "\n",
      "Blue fighter Stats: \n",
      "Shape:  (5062, 76)\n",
      "\n",
      "Red fighter Stats: \n",
      "Shape:  (5062, 76)\n",
      "\n",
      "Total fighter Stats: \n",
      "Shape:  (10124, 75)\n",
      "\n",
      "Fighters no offensive stats: \n",
      "Shape:  (10124, 26)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fight_data_file = '../combined_data/combined_fight_data.csv'\n",
    "\n",
    "def import_and_merge():\n",
    "    fight_data = pd.read_csv(fight_data_file)\n",
    "    \n",
    "    #Add Blue and Red win columns\n",
    "    fight_data['B_Winner'] = [1 if x=='Blue' else 0 for x in fight_data['Winner']]\n",
    "    fight_data['R_Winner'] = [1 if x=='Red' else 0 for x in fight_data['Winner']]\n",
    "    \n",
    "    #Drop columns irrelevant to this prediction\n",
    "    fight_data = fight_data.drop(columns=['Referee', 'no_of_rounds', 'Winner', 'date', 'end_method', 'end_how', \n",
    "                                          'end_round', 'attendance'])\n",
    "    print('Fight Data Stats: ')\n",
    "    print('Shape: ', fight_data.shape)\n",
    "    #display(fight_data)\n",
    "    \n",
    "    #Separate fight data into individual fighter stats\n",
    "    blue_fighters = fight_data.loc[:, [col for col in fight_data.columns if re.search('^R_', col)==None]]\n",
    "    blue_fighters = blue_fighters.rename(columns=lambda x: re.sub('^B_', '', x))\n",
    "    print('\\nBlue fighter Stats: ')\n",
    "    print('Shape: ', blue_fighters.shape)\n",
    "    #display(blue_fighters)\n",
    "    \n",
    "    red_fighters = fight_data.loc[:, [col for col in fight_data.columns if re.search('^B_', col)==None]]\n",
    "    red_fighters = red_fighters.rename(columns=lambda x: re.sub('^R_', '', x))\n",
    "    print('\\nRed fighter Stats: ')\n",
    "    print('Shape: ', red_fighters.shape)\n",
    "    #display(red_fighters)\n",
    "    \n",
    "    #Concatenate blue and red fighter stats\n",
    "    fighters_data = pd.concat([blue_fighters, red_fighters], ignore_index=True)\n",
    "    fighters_data = fighters_data.rename(columns={'total_time_fought(seconds)':'total_time_fought_seconds'})\n",
    "    fighters_data = fighters_data.drop(columns='fighter')\n",
    "    print('\\nTotal fighter Stats: ')\n",
    "    print('Shape: ', fighters_data.shape)\n",
    "    \n",
    "    # Create df without offence stats\n",
    "    fighters_no_offence_stats = fighters_data.drop(columns=[col for col in fighters_data.columns if re.search('^avg_', col) != None])\n",
    "    fighters_no_offence_stats = fighters_no_offence_stats.drop(columns='total_time_fought_seconds')\n",
    "    print('\\nFighters no offensive stats: ')\n",
    "    print('Shape: ', fighters_no_offence_stats.shape)\n",
    "    \n",
    "    return (fighters_no_offence_stats, fighters_data)\n",
    "    \n",
    "data_no_offence_stats, data  = import_and_merge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and Structure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(fighter_df):    \n",
    "   #Fill missing values for height, reach, weight, age\n",
    "    weight_class_means = {}\n",
    "    weight_classes = np.unique(fighter_df['weight_class'])\n",
    "    for weight_class in weight_classes:\n",
    "        weight_class_idx = fighter_df['weight_class'] == weight_class\n",
    "        \n",
    "        null_idx = np.logical_and(pd.isnull(fighter_df['Height_cms']), weight_class_idx)\n",
    "        fighter_df.loc[null_idx, 'Height_cms'] = np.nanmean(fighter_df.loc[weight_class_idx, 'Height_cms'])\n",
    "        \n",
    "        null_idx = np.logical_and(pd.isnull(fighter_df['Reach_cms']), weight_class_idx)\n",
    "        fighter_df.loc[null_idx, 'Reach_cms'] = np.nanmean(fighter_df.loc[weight_class_idx, 'Reach_cms'])\n",
    "        \n",
    "        null_idx = np.logical_and(pd.isnull(fighter_df['Weight_lbs']), weight_class_idx)\n",
    "        fighter_df.loc[null_idx, 'Weight_lbs'] = np.nanmean(fighter_df.loc[weight_class_idx, 'Weight_lbs'])\n",
    "        \n",
    "        null_idx = np.logical_and(pd.isnull(fighter_df['age']), weight_class_idx)\n",
    "        fighter_df.loc[null_idx, 'age'] = np.nanmean(fighter_df.loc[weight_class_idx, 'age'])\n",
    "    \n",
    "    # Fill out missing stance\n",
    "    fighter_df.loc[pd.isnull(fighter_df['Stance']), 'Stance'] = 'Orthodox'\n",
    "    \n",
    "    # Ger rid of fights without location\n",
    "    fighter_df = fighter_df.loc[~pd.isnull(fighter_df['city']), :]\n",
    "    \n",
    "    # Fill missing elevations with 0\n",
    "    fighter_df.loc[pd.isnull(fighter_df['location_elevation']), 'location_elevation'] = 0\n",
    "    fighter_df.loc[pd.isnull(fighter_df['home_elevation']), 'home_elevation'] = 0\n",
    "    \n",
    "    # Replacet title bout with actual numbers\n",
    "    fighter_df.loc[fighter_df['title_bout'] == 'True', 'title_bout'] = 1\n",
    "    fighter_df.loc[fighter_df['title_bout'] == 'False', 'title_bout'] = 0\n",
    "    \n",
    "    #Drop rows with too many missing values\n",
    "    if 'avg_BODY_att' in fighter_df.columns:\n",
    "        fighter_df = fighter_df.loc[~pd.isnull(fighter_df['avg_BODY_att']), :]    \n",
    "        \n",
    "    return fighter_df\n",
    "\n",
    "def structure_data(fighter_df):   \n",
    "    # Split Locations  and hometowns into city and country\n",
    "    if 'location' in fighter_df.columns:\n",
    "        fighter_df['city'] = [str.lower(location.split(', ')[0]) for location in fighter_df['location']]\n",
    "        fighter_df['country'] = [str.lower(location.split(', ')[-1]) for location in fighter_df['location']]\n",
    "        fighter_df = fighter_df.drop(columns='location')\n",
    "    \n",
    "    if 'hometown' in fighter_df.columns:\n",
    "        #First get rid of data with nan hometowns\n",
    "        fighter_df['hometown_city'] = [str.lower(location.split(', ')[0]) for location in fighter_df['hometown']]\n",
    "        fighter_df['hometown_country'] = [str.lower(location.split(', ')[-1]) for location in fighter_df['hometown']]\n",
    "        fighter_df.drop(columns='hometown')\n",
    "        \n",
    "    return fighter_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for data with offence stats\n",
    "def compare_models(knn_params, lr_params, svc_params, nbayes_params, rforest_params, features, labels): # Receives already scales\n",
    "    train_accuracy_df = pd.DataFrame(columns=['Logistic Regression'])\n",
    "    accuracy_df = pd.DataFrame(columns=['Logistic Regression'])\n",
    "    precision_df = pd.DataFrame(columns=['Logistic Regression'])\n",
    "    recall_df = pd.DataFrame(columns=['Logistic Regression'])\n",
    "    f1_df = pd.DataFrame(columns=['Logistic Regression'])\n",
    "    \n",
    "    idx = 0;\n",
    "    kf = KFold(n_splits=7)\n",
    "    \n",
    "    for train_index, test_index in kf.split(features):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.25, random_state=0)\n",
    "        X_train_scaled, X_test_scaled = features.iloc[train_index, :], features.iloc[test_index, :]\n",
    "        y_train, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
    "        \n",
    "        #Models   \n",
    "        lr = LogisticRegression(**lr_params).fit(X_train_scaled, y_train)  \n",
    "        \n",
    "        #Update tables\n",
    "        train_accuracy_df.loc[idx] = lr.score(X_train_scaled, y_train)       \n",
    "        accuracy_df.loc[idx] = lr.score(X_test_scaled, y_test)\n",
    "        precision_df.loc[idx] = precision_score(y_test, lr.predict(X_test_scaled))\n",
    "        recall_df.loc[idx] = recall_score(y_test, lr.predict(X_test_scaled))\n",
    "        \n",
    "        f1_df.loc[idx] = f1_score(y_test, lr.predict(X_test_scaled))\n",
    "        idx += 1\n",
    "        \n",
    "    #Display results\n",
    "    display('------Train accuracy score:-------', train_accuracy_df.median())\n",
    "    display('------Median accuracy score:-------', accuracy_df.median())\n",
    "    display('------Median precision score:------', precision_df.median())\n",
    "    display('------Median recall score:---------', recall_df.median())\n",
    "    display('------Median F1 score:-------------', f1_df.median())\n",
    "    \n",
    "def evaluate_models_best_params(clean_data):\n",
    "    clean_data['Winner'] = data['Winner'].copy()\n",
    "    knn_params = {'algorithm': 'auto', 'n_neighbors': 15}\n",
    "    lr_params = {'C': 0.001, 'solver': 'liblinear'}\n",
    "    svc_params = {'C': 0.05204081639183673}\n",
    "    nbayes_params = {'alpha': 14.571428571459185}\n",
    "    rforest_params = {'max_depth': 10.46938775510204, 'max_features': 'auto', 'min_samples_split': 18}\n",
    "    dummy_df = pd.get_dummies(clean_data)\n",
    "\n",
    "    labels = dummy_df['Winner']\n",
    "    features = dummy_df.drop(columns=['Winner'])\n",
    "    compare_models(knn_params, lr_params, svc_params, nbayes_params, rforest_params, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PERFORMANCE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'------Train accuracy score:-------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Logistic Regression    0.586551\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'------Median accuracy score:-------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Logistic Regression    0.553463\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'------Median precision score:------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Logistic Regression    0.634878\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'------Median recall score:---------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Logistic Regression    0.547658\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'------Median F1 score:-------------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Logistic Regression    0.552897\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = clean_data(data)\n",
    "data = structure_data(data)\n",
    "\n",
    "print('\\nPERFORMANCE')\n",
    "evaluate_models_best_params(data.copy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
