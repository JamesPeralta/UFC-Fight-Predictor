{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Fight Data and Merge with Hometown Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fight Data Stats: \n",
      "Shape:  (5062, 147)\n",
      "\n",
      "Blue fighter Stats: \n",
      "Shape:  (5062, 76)\n",
      "\n",
      "Red fighter Stats: \n",
      "Shape:  (5062, 76)\n",
      "\n",
      "Total fighter Stats: \n",
      "Shape:  (10124, 75)\n",
      "\n",
      "Fighters no offensive stats: \n",
      "Shape:  (10124, 26)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fight_data_file = '../generated_data/combined_fight_data.csv'\n",
    "\n",
    "def import_and_merge():\n",
    "    fight_data = pd.read_csv(fight_data_file)\n",
    "    \n",
    "    #Add Blue and Red win columns\n",
    "    fight_data['B_Winner'] = [1 if x=='Blue' else 0 for x in fight_data['Winner']]\n",
    "    fight_data['R_Winner'] = [1 if x=='Red' else 0 for x in fight_data['Winner']]\n",
    "    \n",
    "    #Drop columns irrelevant to this prediction\n",
    "    fight_data = fight_data.drop(columns=['Referee', 'no_of_rounds', 'Winner', 'date', 'end_method', 'end_how', \n",
    "                                          'end_round', 'attendance'])\n",
    "    print('Fight Data Stats: ')\n",
    "    print('Shape: ', fight_data.shape)\n",
    "    #display(fight_data)\n",
    "    \n",
    "    #Separate fight data into individual fighter stats\n",
    "    blue_fighters = fight_data.loc[:, [col for col in fight_data.columns if re.search('^R_', col)==None]]\n",
    "    blue_fighters = blue_fighters.rename(columns=lambda x: re.sub('^B_', '', x))\n",
    "    print('\\nBlue fighter Stats: ')\n",
    "    print('Shape: ', blue_fighters.shape)\n",
    "    #display(blue_fighters)\n",
    "    \n",
    "    red_fighters = fight_data.loc[:, [col for col in fight_data.columns if re.search('^B_', col)==None]]\n",
    "    red_fighters = red_fighters.rename(columns=lambda x: re.sub('^R_', '', x))\n",
    "    print('\\nRed fighter Stats: ')\n",
    "    print('Shape: ', red_fighters.shape)\n",
    "    #display(red_fighters)\n",
    "    \n",
    "    #Concatenate blue and red fighter stats\n",
    "    fighters_data = pd.concat([blue_fighters, red_fighters], ignore_index=True)\n",
    "    fighters_data = fighters_data.rename(columns={'total_time_fought(seconds)':'total_time_fought_seconds'})\n",
    "    fighters_data = fighters_data.drop(columns='fighter')\n",
    "    print('\\nTotal fighter Stats: ')\n",
    "    print('Shape: ', fighters_data.shape)\n",
    "    \n",
    "    # Create df without offence stats\n",
    "    fighters_no_offence_stats = fighters_data.drop(columns=[col for col in fighters_data.columns if re.search('^avg_', col) != None])\n",
    "    fighters_no_offence_stats = fighters_no_offence_stats.drop(columns='total_time_fought_seconds')\n",
    "    print('\\nFighters no offensive stats: ')\n",
    "    print('Shape: ', fighters_no_offence_stats.shape)\n",
    "    \n",
    "    return (fighters_no_offence_stats, fighters_data)\n",
    "    \n",
    "data_no_offence_stats, data  = import_and_merge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data cleaning and model selection\n",
    "- Separate location and home town into city and country\n",
    "- Categorize weight_class, title_bout, Stance, and locations\n",
    "- Convert elevation data into numeric data\n",
    "- Tune parameters\n",
    "- Visualize model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def plot_missing_vals(df):\n",
    "    columns = []\n",
    "    nans_per_col = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        num_nans = sum(pd.isnull(df[col]))\n",
    "        #print('Num of NaNs in col ', col, ': ', num_nans)\n",
    "        columns.append(col)\n",
    "        nans_per_col.append(num_nans)\n",
    "    \n",
    "    print('Shape: ', df.shape)\n",
    "    fig = plt.figure(figsize=(30, 13))\n",
    "    ax = plt.axes()\n",
    "    ax.bar(columns, nans_per_col)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.ylabel('# NaNs')\n",
    "    plt.title('Number of missing data per feature')\n",
    "    \n",
    "def inspect_data(fighter_df):\n",
    "    # Display missing value numbers in fighter data\n",
    "    print('\\nBefore dropping fightres with no offence/defence stats')\n",
    "    plot_missing_vals(fighter_df)\n",
    "    \n",
    "    #Fighter age, weight, height, and reach distribution\n",
    "    hrw_attrs_df = pd.DataFrame(fighter_df[['Height_cms', 'Reach_cms', 'Weight_lbs']])    \n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    ax = plt.axes()\n",
    "    hrw_attrs_df.boxplot()\n",
    "    plt.title('Height, Reach and weight dsitributions')\n",
    "    plt.ylim([100, 250])\n",
    "    \n",
    "    age_df = pd.DataFrame(fighter_df[['age']])   \n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    ax = plt.axes()\n",
    "    age_df.boxplot()\n",
    "    plt.title('Age dsitributions')\n",
    "       \n",
    "\n",
    "def clean_data(fighter_df):    \n",
    "   #Fill missing values for height, reach, weight, age\n",
    "    weight_class_means = {}\n",
    "    weight_classes = np.unique(fighter_df['weight_class'])\n",
    "    for weight_class in weight_classes:\n",
    "        weight_class_idx = fighter_df['weight_class'] == weight_class\n",
    "        \n",
    "        null_idx = np.logical_and(pd.isnull(fighter_df['Height_cms']), weight_class_idx)\n",
    "        fighter_df.loc[null_idx, 'Height_cms'] = np.nanmean(fighter_df.loc[weight_class_idx, 'Height_cms'])\n",
    "        \n",
    "        null_idx = np.logical_and(pd.isnull(fighter_df['Reach_cms']), weight_class_idx)\n",
    "        fighter_df.loc[null_idx, 'Reach_cms'] = np.nanmean(fighter_df.loc[weight_class_idx, 'Reach_cms'])\n",
    "        \n",
    "        null_idx = np.logical_and(pd.isnull(fighter_df['Weight_lbs']), weight_class_idx)\n",
    "        fighter_df.loc[null_idx, 'Weight_lbs'] = np.nanmean(fighter_df.loc[weight_class_idx, 'Weight_lbs'])\n",
    "        \n",
    "        null_idx = np.logical_and(pd.isnull(fighter_df['age']), weight_class_idx)\n",
    "        fighter_df.loc[null_idx, 'age'] = np.nanmean(fighter_df.loc[weight_class_idx, 'age'])\n",
    "    \n",
    "    # Fill out missing stance\n",
    "    fighter_df.loc[pd.isnull(fighter_df['Stance']), 'Stance'] = 'Orthodox'\n",
    "    \n",
    "    # Ger rid of fights without location\n",
    "    fighter_df = fighter_df.loc[~pd.isnull(fighter_df['city']), :]\n",
    "    \n",
    "    # Fill missing elevations with 0\n",
    "    fighter_df.loc[pd.isnull(fighter_df['location_elevation']), 'location_elevation'] = 0\n",
    "    fighter_df.loc[pd.isnull(fighter_df['home_elevation']), 'home_elevation'] = 0\n",
    "    \n",
    "    # Replacet title bout with actual numbers\n",
    "    fighter_df.loc[fighter_df['title_bout'] == 'True', 'title_bout'] = 1\n",
    "    fighter_df.loc[fighter_df['title_bout'] == 'False', 'title_bout'] = 0\n",
    "    \n",
    "    #Drop rows with too many missing values\n",
    "    if 'avg_BODY_att' in fighter_df.columns:\n",
    "        fighter_df = fighter_df.loc[~pd.isnull(fighter_df['avg_BODY_att']), :]    \n",
    "        \n",
    "    return fighter_df\n",
    "\n",
    "def structure_data(fighter_df):   \n",
    "    # Split Locations  and hometowns into city and country\n",
    "    if 'location' in fighter_df.columns:\n",
    "        fighter_df['city'] = [str.lower(location.split(', ')[0]) for location in fighter_df['location']]\n",
    "        fighter_df['country'] = [str.lower(location.split(', ')[-1]) for location in fighter_df['location']]\n",
    "        fighter_df = fighter_df.drop(columns='location')\n",
    "    \n",
    "    if 'hometown' in fighter_df.columns:\n",
    "        #First get rid of data with nan hometowns\n",
    "        fighter_df['hometown_city'] = [str.lower(location.split(', ')[0]) for location in fighter_df['hometown']]\n",
    "        fighter_df['hometown_country'] = [str.lower(location.split(', ')[-1]) for location in fighter_df['hometown']]\n",
    "        fighter_df.drop(columns='hometown')\n",
    "        \n",
    "    return fighter_df\n",
    "    \n",
    "\n",
    "def compare_models(knn_params, lr_params, svc_params, nbayes_params, rforest_params, features, labels): # Receives already scales\n",
    "    train_accuracy_df = pd.DataFrame(columns=['KNN', 'Logistic Regression', 'SVC', 'Naive Bayes', 'Random Forest'])\n",
    "    accuracy_df = pd.DataFrame(columns=['KNN', 'Logistic Regression', 'SVC', 'Naive Bayes', 'Random Forest'])\n",
    "    precision_df = pd.DataFrame(columns=['KNN', 'Logistic Regression', 'SVC', 'Naive Bayes', 'Random Forest'])\n",
    "    recall_df = pd.DataFrame(columns=['KNN', 'Logistic Regression', 'SVC', 'Naive Bayes', 'Random Forest'])\n",
    "    f1_df = pd.DataFrame(columns=['KNN', 'Logistic Regression', 'SVC', 'Naive Bayes', 'Random Forest'])\n",
    "    \n",
    "    idx = 0;\n",
    "    kf = KFold(n_splits=20)\n",
    "    \n",
    "    for train_index, test_index in kf.split(features):\n",
    "        X_train_scaled, X_test_scaled = features.iloc[train_index, :], features.iloc[test_index, :]\n",
    "        y_train, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
    "        \n",
    "        #Models\n",
    "        knn = KNeighborsClassifier(**knn_params).fit(X_train_scaled, y_train)    \n",
    "        lr = LogisticRegression(**lr_params).fit(X_train_scaled, y_train)  \n",
    "        svc = LinearSVC(**svc_params).fit(X_train_scaled, y_train)\n",
    "        n_bayes = BernoulliNB(**nbayes_params).fit(X_train_scaled, y_train)\n",
    "        random_forest = RandomForestClassifier(n_estimators=50, **rforest_params).fit(X_train_scaled, y_train)\n",
    "        \n",
    "        #Update tables\n",
    "        train_accuracy_df.loc[idx] = [knn.score(X_train_scaled, y_train), lr.score(X_train_scaled, y_train), svc.score(X_train_scaled, y_train), \n",
    "                                n_bayes.score(X_train_scaled, y_train), random_forest.score(X_train_scaled, y_train)]\n",
    "        \n",
    "        accuracy_df.loc[idx] = [knn.score(X_test_scaled, y_test), lr.score(X_test_scaled, y_test), svc.score(X_test_scaled, y_test), \n",
    "                                n_bayes.score(X_test_scaled, y_test), random_forest.score(X_test_scaled, y_test)]\n",
    "\n",
    "        precision_df.loc[idx] = [precision_score(y_test, knn.predict(X_test_scaled)), precision_score(y_test, lr.predict(X_test_scaled)), \n",
    "                                 precision_score(y_test, svc.predict(X_test_scaled)), precision_score(y_test, n_bayes.predict(X_test_scaled)),\n",
    "                                 precision_score(y_test, random_forest.predict(X_test_scaled))]\n",
    "\n",
    "        recall_df.loc[idx] = [recall_score(y_test, knn.predict(X_test_scaled)), recall_score(y_test, lr.predict(X_test_scaled)), \n",
    "                              recall_score(y_test, svc.predict(X_test_scaled)), recall_score(y_test, n_bayes.predict(X_test_scaled)),\n",
    "                              recall_score(y_test, random_forest.predict(X_test_scaled))]\n",
    "\n",
    "        f1_df.loc[idx] = [f1_score(y_test, knn.predict(X_test_scaled)), f1_score(y_test, lr.predict(X_test_scaled)), \n",
    "                          f1_score(y_test, svc.predict(X_test_scaled)), f1_score(y_test, n_bayes.predict(X_test_scaled)),\n",
    "                          f1_score(y_test, random_forest.predict(X_test_scaled))]\n",
    "        idx += 1\n",
    "        \n",
    "    #Display results\n",
    "    display('------Train accuracy score:-------', train_accuracy_df.median())\n",
    "    display('------Median accuracy score:-------', accuracy_df.median())\n",
    "    display('------Median precision score:------', precision_df.median())\n",
    "    display('------Median recall score:---------', recall_df.median())\n",
    "    display('------Median F1 score:-------------', f1_df.median())\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    train_accuracy_df.drop(columns='SVC').boxplot()\n",
    "    plt.title('Train Accuracy scores')\n",
    "    plt.ylabel('Score')\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    accuracy_df.drop(columns='SVC').boxplot()\n",
    "    plt.title('Accuracy scores')\n",
    "    plt.ylabel('Score')\n",
    "     \n",
    "    fig = plt.figure(figsize=(15, 7))       \n",
    "    precision_df.drop(columns='SVC').boxplot()\n",
    "    plt.title('Precision score')\n",
    "    plt.ylabel('Score')\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 7))        \n",
    "    recall_df.drop(columns='SVC').boxplot()\n",
    "    plt.title('Recall score')\n",
    "    plt.ylabel('Score')\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 7))        \n",
    "    f1_df.drop(columns='SVC').boxplot()\n",
    "    plt.title('F1 score')\n",
    "    plt.ylabel('Score')\n",
    "    \n",
    "    \n",
    "def parameter_tuning(df, scale):\n",
    "    df = structure_data(df)\n",
    "    df = clean_data(df)\n",
    "    dummy_df = pd.get_dummies(df)\n",
    "    \n",
    "    labels = dummy_df['Winner']\n",
    "    features = dummy_df.drop(columns=['Winner'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, train_size=0.75, test_size=0.25, random_state=0, stratify=labels)\n",
    "    \n",
    "    #Scale\n",
    "    if scale:\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train_scaled = X_train\n",
    "        X_test_scaled = X_test\n",
    "\n",
    "    \n",
    "    # Search for best parameters\n",
    "    knn = KNeighborsClassifier().fit(X_train_scaled, y_train)\n",
    "    knn_tuner = GridSearchCV(knn, n_jobs=-1, param_grid={'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'], 'n_neighbors':list(range(1,51))}).fit(X_train_scaled, y_train)\n",
    "    print('KNN Best params:', knn_tuner.best_params_ )\n",
    "    \n",
    "    lr = LogisticRegression().fit(X_train_scaled, y_train)\n",
    "    lr_tuner =  GridSearchCV(lr, n_jobs=-1, param_grid={'solver':['newton-cg', 'lbfgs', 'liblinear'], 'C':list(np.linspace(0.001,20,40))}).fit(X_train_scaled, y_train)\n",
    "    print('LR Best params:', lr_tuner.best_params_)\n",
    "    \n",
    "    svc = LinearSVC().fit(X_train_scaled, y_train)\n",
    "    svc_tuner = GridSearchCV(svc, n_jobs=-1, param_grid={'C':list(np.linspace(0.0000000001, 0.15, 50))}).fit(X_train_scaled, y_train)\n",
    "    print('SVC Best params:', svc_tuner.best_params_)\n",
    "    \n",
    "    n_bayes = BernoulliNB().fit(X_train_scaled, y_train)\n",
    "    nbayes_tuner = GridSearchCV(n_bayes, n_jobs=-1, param_grid={'alpha':list(np.linspace(0.0000000001, 21, 50))}).fit(X_train_scaled, y_train)\n",
    "    print('NaiveBayes Best params:', nbayes_tuner.best_params_)\n",
    "    \n",
    "    random_forest = RandomForestClassifier(n_estimators=100, random_state=0).fit(X_train_scaled, y_train)\n",
    "    rforest_tuner = GridSearchCV(random_forest, n_jobs=-1, param_grid={'max_features':['auto', 'sqrt', 'log2'], 'max_depth':list(np.linspace(1,30,50)), 'min_samples_split':list(range(2,30))}).fit(X_train_scaled, y_train)\n",
    "    print('RandomForest Best params:', rforest_tuner.best_params_)\n",
    "    \n",
    "    # Compare models with tuned parameters\n",
    "    compare_models( knn_tuner.best_params_, lr_tuner.best_params_, svc_tuner.best_params_, nbayes_tuner.best_params_, rforest_tuner.best_params_, features, labels)\n",
    "    \n",
    "    return(knn_tuner.best_params_, lr_tuner.best_params_, svc_tuner.best_params_, nbayes_tuner.best_params_, rforest_tuner.best_params_, features, labels)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Best params: {'algorithm': 'ball_tree', 'n_neighbors': 7}\n",
      "LR Best params: {'C': 10.769692307692305, 'solver': 'lbfgs'}\n",
      "SVC Best params: {'C': 0.1469387755122449}\n",
      "NaiveBayes Best params: {'alpha': 21.0}\n"
     ]
    }
   ],
   "source": [
    "# Tune parameters for data with no offence stats\n",
    "knn_params, lr_params, svc_params, nbayes_params, rforest_params, features, labels = parameter_tuning(data_no_offence_stats, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune parameters for data with offence stats\n",
    "knn_params_offence, lr_params_offence, svc_params_offence, nbayes_params_offence, rforest_params_offence, features_offence, labels_offence = parameter_tuning(data, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_models_best_params_no_offence(data):\n",
    "    data['Winner'] = data_no_offence_stats['Winner'].copy()\n",
    "    knn_params =  {'algorithm': 'ball_tree', 'n_neighbors': 7}\n",
    "    lr_params = {'C': 7.180128205128204, 'solver': 'lbfgs'}\n",
    "    svc_params = {'C': 0.006122449075510203}\n",
    "    nbayes_params =  {'alpha': 21.0}\n",
    "    rforest_params = {'max_depth': 10.46938775510204, 'max_features': 'auto', 'min_samples_split': 15}\n",
    "    \n",
    "    dummy_df = pd.get_dummies(data)\n",
    "\n",
    "    labels = dummy_df['Winner']\n",
    "    features = dummy_df.drop(columns=['Winner'])\n",
    "    compare_models(knn_params, lr_params, svc_params, nbayes_params, rforest_params, features, labels)\n",
    "\n",
    "\n",
    "def evaluate_models_best_params(clean_data):\n",
    "    clean_data['Winner'] = data['Winner'].copy()\n",
    "    knn_params = {'algorithm': 'auto', 'n_neighbors': 7}\n",
    "    lr_params = {'C': 0.001, 'solver': 'liblinear'}\n",
    "    svc_params = {'C': 0.04897959190408163}\n",
    "    nbayes_params = {'alpha': 19.714285714291837}\n",
    "    rforest_params = {'max_depth': 3.36734693877551, 'max_features': 'auto', 'min_samples_split': 10}\n",
    "    \n",
    "    dummy_df = pd.get_dummies(clean_data)\n",
    "\n",
    "    labels = dummy_df['Winner']\n",
    "    features = dummy_df.drop(columns=['Winner'])\n",
    "    compare_models(knn_params, lr_params, svc_params, nbayes_params, rforest_params, features, labels)\n",
    "\n",
    "# Explore correlation and find best correlated features\n",
    "def plot_correlation(data_no_offence_stats):\n",
    "    df = data_no_offence_stats.drop(columns=['city', 'country', 'weight_class'])\n",
    "\n",
    "    corr = df.corr()['Winner'][:]\n",
    "    corr = corr.drop(['Winner']) * 100\n",
    "    corr = corr.loc[~pd.isnull(corr)]\n",
    "    corr = abs(corr).sort_values()\n",
    "\n",
    "    most_corr_features = corr.index[-int(len(corr)/2):]\n",
    "    return list(most_corr_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Different feature-combinations with the best parameters obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "data_no_offence_stats = clean_data(data_no_offence_stats)\n",
    "data = clean_data(data)\n",
    "\n",
    "# Grab most correlated features\n",
    "most_corr_features_no_offence = plot_correlation(data_no_offence_stats)\n",
    "most_corr_features = plot_correlation(data)\n",
    "\n",
    "data_no_offence_stats = structure_data(data_no_offence_stats)\n",
    "data = structure_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('************FEATURE COMB: ALL FEATURES**************')\n",
    "evaluate_models_best_params(data.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('************FEATURE COMB: DATA WITHOUT OFFENCE STATS**************')\n",
    "evaluate_models_best_params_no_offence(data_no_offence_stats.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*****TOP CORRELATED FEATURES FROM FEATURE COMB-ALL FEATURES********')\n",
    "evaluate_models_best_params(data.loc[:,most_corr_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*****TOP CORRELATED FEATURES FROM FEATURE COMB-DATA WITHHOYT OFFENCE STATSS********')\n",
    "evaluate_models_best_params_no_offence(data_no_offence_stats.loc[:,most_corr_features_no_offence].copy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
